{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Resources\n",
        "\n",
        "- https://developmentseed.org/lonboard/latest/ (lonboard)\n",
        "- https://pygis.io/docs/g_hire.html\n",
        "\n",
        " -->\n",
        "\n",
        "# GIS\n",
        "\n",
        "Spatial data refers to information that has a geographic or locational component. It encompasses data that describes the physical location and characteristics of objects in space, such as points, lines, polygons, and surfaces. Spatial data is essential for Geographic Information Systems (GIS), which are tools used for capturing, storing, analyzing, and visualizing spatial data to understand patterns, relationships, and trends in the real world.\n",
        "\n",
        "Python has emerged as a popular programming language for GIS due to its versatility, ease of use, and extensive libraries specifically designed for spatial data analysis and visualization. Python offers powerful tools like GeoPandas, Shapely, Fiona, and Pyproj, which facilitate tasks such as reading, manipulating, and processing spatial data. Additionally, Python's integration with other libraries and frameworks, such as Matplotlib, Pandas, and NumPy, further enhances its capabilities for GIS applications. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# requirementss\n",
        "# %pip install geopandas matplotlib shapely rasterio numpy pandas sklearn-xarray -q\n",
        "# %pip install git+https://github.com/jgrss/geowombat  -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spatial Data\n",
        "\n",
        "\n",
        "To work in a GIS environment, real world observations (objects or events that can be recorded in 2D or 3D space) need to be reduced to spatial entities. These spatial entities can be represented in a GIS as a `vector data model` or a `raster data model`.\n",
        "\n",
        "\n",
        "Below there is the representation of a river in the 2 formats, vector (left) and raster (right)\n",
        "\n",
        "![](https://pygis.io/_images/vector_vs_raster.jpg)\n",
        "\n",
        "### Vector Data\n",
        "\n",
        "Vector features can be decomposed into three different geometric primitives: `points`, `polylines` and `polygons`.\n",
        "\n",
        "For each of these examples, we’ll be using a Python dictionary to form the basis of a GeoDataFrame. A dictionary in Python is a data structure that allows you to store data as pairs of keys and values. \n",
        "\n",
        "\n",
        "#### Point\n",
        "\n",
        "We can use the geopandas and shapely libraries to create a GeoDataFrame that contains point data.\n",
        "\n",
        "A point is composed of one coordinate pair representing a specific location in a coordinate system. Points are the most basic geometric primitives having no length or area. By definition a point can’t be “seen” since it has no area; but this is not practical if such primitives are to be mapped. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.geometry import Point\n",
        "\n",
        "d = {\n",
        "    \"name\": [\n",
        "        \"Washington\\n(38.90, -77.03)\",\n",
        "        \"Baltimore\\n(39.29, -76.61)\",\n",
        "        \"Fredrick\\n(39.41,-77.40)\",\n",
        "    ],\n",
        "    \"geometry\": [\n",
        "        Point(-77.036873, 38.907192),\n",
        "        Point(\n",
        "            -76.612190,\n",
        "            39.290386,\n",
        "        ),\n",
        "        Point(-77.408456, 39.412006),\n",
        "    ],\n",
        "}\n",
        "\n",
        "gdf = gpd.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
        "print(gdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use `matplotlib` to plot our `GeoDataFrame`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.style.use('bmh') # better for plotting geometries vs general plots.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "gdf.plot(ax=ax)\n",
        "plt.ylim([38.8, 39.6])\n",
        "plt.xlim([-77.5, -76.2])\n",
        "\n",
        "for x, y, label in zip(gdf.geometry.x, gdf.geometry.y, gdf.name):\n",
        "    ax.annotate(label, xy=(x, y), xytext=(3, 3), textcoords=\"offset points\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Line String\n",
        "\n",
        "Linestring is composed of a sequence of two or more coordinate pairs called vertices. A vertex is defined by coordinate pairs, just like a point, but what differentiates a vertex from a point is its explicitly defined relationship with neighboring vertices.\n",
        "\n",
        "\n",
        "We can create a GeoDataFrame like a `GeoJson`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import required modules\n",
        "from shapely.geometry import LineString, Point\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a dictionary with names and geometry (LineString)\n",
        "d = {\n",
        "    \"name\": [\"Washington\\n(38.90, -77.03)\"],\n",
        "    \"geometry\": [\n",
        "        LineString(\n",
        "            [\n",
        "                Point(-77.036873, 38.907192),\n",
        "                Point(\n",
        "                    -76.612190,\n",
        "                    39.290386,\n",
        "                ),\n",
        "                Point(-77.408456, 39.412006),\n",
        "            ]\n",
        "        )\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Create a GeoDataFrame using the dictionary\n",
        "gdf = gpd.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
        "\n",
        "# Plot the GeoDataFrame\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "gdf.plot(ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Polygon\n",
        "\n",
        "A polygon, in geographic information systems (GIS), is an area defined by three or more line segments, each with a starting and ending pair of coordinates that match. The terms lattice or area might be used interchangeably with ‘polygon’. A polygon’s defining features include its length (i.e., the perimeter), its area, and the concept of an interior and exterior. Specifically in GIS, the enclosed area of a polygon is explicitly defined.\n",
        "\n",
        "\n",
        "Note that we explicitly have to call the `Polygon` method in order to create the polygon area, just like in the previous cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from shapely.geometry import Polygon\n",
        "\n",
        "d = {\n",
        "    \"name\": [\"Washington\\n(38.90, -77.03)\"],\n",
        "    \"geometry\": [\n",
        "        Polygon(\n",
        "            [\n",
        "                Point(-77.036873, 38.907192),\n",
        "                Point(\n",
        "                    -76.612190,\n",
        "                    39.290386,\n",
        "                ),\n",
        "                Point(-77.408456, 39.412006),\n",
        "            ]\n",
        "        )\n",
        "    ],\n",
        "}\n",
        "gdf = gpd.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "gdf.plot(ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In other cases we can plot the graph quickly by placing `.plot()` at the end of the GeoDataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Native Plot\n",
        "\n",
        "gdf.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Raster Data\n",
        "\n",
        "\n",
        "Raster data models utilize a grid of cells, known as pixels, to depict real-world objects like imagery, surface temperatures, and elevation models. These grids resemble regularly spaced arrays of marked points, with each cell in the grid having its own set of coordinates. Rasters are commonly stored as arrays of values in GIS environments.\n",
        "\n",
        "A crucial feature of raster data models is that each cell or pixel has an associated value. This stands in contrast to vector models where the geometric primitive may or may not have an associated value.\n",
        "\n",
        "\n",
        "To show what a raster looks like we can create a 2-dimensional array and show the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "X=np.random.randint(256, size=(10, 10))\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "plt.imshow(X)\n",
        "plt.title(\"Plot 2D array\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a more practical example, we can use the library rasterio to handle geospatial raster data. Rasterio reads and writes geospatial raster datasets, making it an essential tool for raster analysis. The following code reads a raster dataset and then plots it:\n",
        "\n",
        "To better exemplify we can use the data that is hosted online from the package [`rasterio`](https://github.com/rasterio/rasterio/tree/main)\n",
        "\n",
        "It’s important to use a context manager (`with` statement) when opening files with Rasterio to ensure they’re properly closed after use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open a web location with a demo raster dataset\n",
        "url = 'https://github.com/mapbox/rasterio/raw/master/tests/data/goes.tif'\n",
        "with rasterio.open(url) as src:\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    show(src.read(), ax=ax, transform=src.transform)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spatial Data File Formats\n",
        "\n",
        "<!-- https://pygis.io/docs/c_store_features.html#raster-data-file-formats -->\n",
        "\n",
        "### Vector Data Files\n",
        "\n",
        "For this type of files, Geopandas is commonly used, and some formats supported by Geopandas include:\n",
        "\n",
        "- `.GeoJson`: GeoJSON is an open standard format specifically designed for representing simple geographical features, along with their non-spatial attributes, based on JavaScript Object Notation (JSON). You can create geojson with following page [`geojson.io`](https://geojson.io/)\n",
        "- GeoPackage [`.gpkg`]: GeoPackage is a modern, open standard data format that adopts the principles of open format standards, ensuring non-proprietary access and wide compatibility. It leverages SQLite, a widely used, self-contained, and serverless transactional SQL database engine, making it an exceptionally portable and lightweight choice for data storage.\n",
        "- ShapeFile [`.shp`]: A Shapefile is a prevalent, file-based data format that was originally associated with ArcView 3.x software, a predecessor to the modern ArcMap and ArcGIS. In essence, a shapefile is akin to a feature class – it stores a set of features that share a common geometry type (point, line, or polygon), possess the same attributes, and occupy a shared spatial extent.\n",
        "\n",
        "\n",
        "### Raster Data File Formats \n",
        "\n",
        "Raster data presents a continuous view of geographical features through a matrix of cells or pixels, each holding a specific value representing attributes like elevation or temperature. This structure is ideal for portraying large, continuous surfaces and gradual attribute changes. Pixel depth is crucial in raster data, as it determines the range of distinct values a pixel can hold, with higher depths enabling more detailed representations but demanding increased storage space.\n",
        "\n",
        "![Pixel depth allows for a wider range of values but also takes up more space](https://pygis.io/_images/raster_storage.png)\n",
        "\n",
        "\n",
        "- Image [`.img`]: The Imagine file format, developed by the image processing software company ERDAS, is a straightforward format represented by a single `.img` file. It is simpler than the shapefile format and often comes with an associated `.xml `file, which typically contains metadata about the raster layer.\n",
        "\n",
        "- GeoTiff [`.tiff`]: GeoTIFF is a widely-used raster data format in the public domain. It is a great choice when you prioritize portability and platform independence as it embeds spatial information within the TIFF image file itself.\n",
        "\n",
        "[Example datasets](https://github.com/rasterio/rasterio/tree/main/tests/data)\n",
        "\n",
        "## Spatial Raster Data in Python\n",
        "\n",
        "<!-- https://pygis.io/docs/c_rasters.html#spatial-raster-data-in-python -->\n",
        "\n",
        "A raster data model utilizes an array of cells or pixels to represent various real-world objects such as imagery, elevation models, and temperatures. Rasters are essentially a grid of cells where each cell holds a specific value, making them suitable for depicting continuous surfaces. Unlike vector data models, raster data is inherently associated with values for each pixel. Tools like rasterio and geowombat, which utilize numpy.ndarray, are commonly used to work with raster data in GIS environments. Constructing a raster from scratch can aid in understanding its workings.\n",
        "\n",
        "Here we create two ndarray objects one X spans [-90°,90°] longitude, and Y covers [-90°,90°] latitude."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "x = np.linspace(-90, 90, 6)\n",
        "y = np.linspace(90, -90, 6)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s generate some data representing temperature and store it an array Z"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Z1 =  np.abs(((X - 10) ** 2 + (Y - 10) ** 2) / 1 ** 2)\n",
        "Z2 =  np.abs(((X + 10) ** 2 + (Y + 10) ** 2) / 2.5 ** 2)\n",
        "Z =  (Z1 - Z2)\n",
        "\n",
        "plt.imshow(Z)\n",
        "plt.title(\"Temperature\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that `Z` contains no data on its location. Its just an array, the information stored in `x` and `y` aren’t associated with it at all. \n",
        "\n",
        "\n",
        "\n",
        "### Creating a spatial data with `numpy` and `rasterio`\n",
        "\n",
        "We need three components:\n",
        "\n",
        "- An array of data and typically the xy coordinates\n",
        "- A coordinate reference system which defines what coordinate space we are using (e.g. degrees or meters, where is the prime meridian etc)\n",
        "- A transform defining the coordinate of the upper left hand corner and the cell resolution\n",
        "\n",
        "Once we have those components, we can write a functional spatial raster dataset in Python in a few lines of code. We only need to provide the information mentioned above in a format that Rasterio understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from rasterio.transform import Affine\n",
        "import rasterio as rio, os\n",
        "\n",
        "res = (x[-1] - x[0]) / 240.0\n",
        "transform = Affine.translation(x[0] - res / 2, y[0] - res / 2) * Affine.scale(res, res)\n",
        "\n",
        "\n",
        "master_dir = \"./temp/\"\n",
        "if not os.path.exists(master_dir):\n",
        "    os.makedirs(master_dir)\n",
        "\n",
        "# open in 'write' mode, unpack profile info to dst\n",
        "with rio.open(\n",
        "    \"./temp/new_raster.tif\",\n",
        "    \"w\",\n",
        "    driver=\"GTiff\",         # output file type\n",
        "    height=Z.shape[0],      # shape of array\n",
        "    width=Z.shape[1],\n",
        "    count=1,                # number of bands\n",
        "    dtype=Z.dtype,          # output datatype\n",
        "    crs=\"+proj=latlong\",    # CRS\n",
        "    transform=transform,    # location and resolution of upper left cell\n",
        ") as dst:\n",
        "    # check for number of bands\n",
        "    if dst.count == 1:\n",
        "        # write single band\n",
        "        dst.write(Z, 1)\n",
        "    else:\n",
        "        # write each band individually\n",
        "        for band in range(len(Z)):\n",
        "            # write data, band # (starting from 1)\n",
        "            dst.write(Z[band], band + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.listdir(master_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Coordinate Reference Systems (CRS)\n",
        "\n",
        "<!-- https://pygis.io/docs/d_vector_crs_intro.html#vector-coordinate-reference-systems-crs -->\n",
        "\n",
        "\n",
        "Coordinate reference systems work well with points, lines, and polygons because each point or node has a specific set of coordinates (x, y). The challenge lies in understanding how these coordinates correspond to real-world locations on the ground.\n",
        "\n",
        "When you create a point, line, or polygon, each point or node is defined by two coordinates, x and y. However, these coordinates can represent vastly different locations on the ground depending on the projection used. For example, the coordinate pair (0,0) could correspond to a location near Ghana in the WGS84 LatLon projection, or it could be in the middle of the Pacific Ocean under another projection.\n",
        "\n",
        "Consider the example of a polygon with coordinates (0,45),(5,45),(5,40),(0,40). In the left panel, these coordinates are provided, but without a specified coordinate reference system, their actual locations are unknown. They could be anywhere, even in outer space. However, in the right panel, when we assign the WGS84 geographic lat lon projection (EPSG:4326), the coordinates suddenly become meaningful because we understand how they relate to real-world locations on the ground.\n",
        "\n",
        "\n",
        "![Example of assigning a coordinate reference system](https://pygis.io/_images/d_crs_assigned.png)\n",
        "\n",
        "\n",
        "Every time we create vector data (or receive it from someone else), we need to make sure that a projection is assigned to it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, Polygon\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"bmh\")\n",
        "\n",
        "crs_latlong = \"EPSG:4326\"\n",
        "\n",
        "a_square = {\n",
        "    \"name\": [\"Washington\\n(38.90, -77.03)\"],\n",
        "    \"geometry\": [Polygon([Point(0, 45), Point(5, 45), Point(5, 40), Point(0, 40)])],\n",
        "}\n",
        "\n",
        "# create a dataframe from the nodes and assign the CRS\n",
        "gdf_square = gpd.GeoDataFrame(a_square, crs=crs_latlong)\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "gdf_square.plot(ax=ax)\n",
        "plt.ylim(38, 50)\n",
        "plt.xlim(0, 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ReProject Points, Lines, Polygons\n",
        "\n",
        "\n",
        "When we need to change the projection of a dataset, we undergo a process called \"reprojection.\" This involves converting coordinates (x, y) stored in latitude and longitude into another coordinate system using a set of formulas. It's a two-step process.\n",
        "\n",
        "For instance, let's consider moving from the Sinosoidal projection to the Hobo-Dyer projection. First, we use the \"inverse equation\" to convert coordinate pairs from Sinosoidal back to latitude and longitude. Then, we use the forward equation to convert latitude and longitude into the Hobo-Dyer coordinate system.\n",
        "\n",
        "![Reprojecting vectors](https://pygis.io/_images/d_reprojection_example.jpg)\n",
        "\n",
        "\n",
        "We can use geopandas to move the prime meridian 10 degrees west"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# reproject the data\n",
        "gdf_square_10w = gdf_square.to_crs(\"+proj=longlat +datum=WGS84 +lon_0=-10\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "gdf_square_10w.plot(ax=ax)\n",
        "plt.ylim(38, 50)\n",
        "plt.xlim(0, 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ReProject a Raster\n",
        "\n",
        "### Reprojecting Raster Data with Rasterio\n",
        "\n",
        "To reproject raster data using Rasterio, follow these steps:\n",
        "\n",
        "1. Import the required libraries:\n",
        "   \n",
        "   ```python\n",
        "   import numpy as np\n",
        "   import rasterio\n",
        "   from rasterio.warp import reproject, Resampling, calculate_default_transform\n",
        "   ```\n",
        "\n",
        "2. Define the destination CRS (Coordinate Reference System). For example, if you want to reproject to Web Mercator (EPSG:3857), set:\n",
        "   \n",
        "   ```python\n",
        "   dst_crs = \"EPSG:3857\"\n",
        "   ```\n",
        "\n",
        "3. Specify the URL of the raster file you want to reproject:\n",
        "   \n",
        "   ```python\n",
        "   url = \"https://github.com/rasterio/rasterio/raw/main/tests/data/RGB.byte.tif\"\n",
        "   ```\n",
        "\n",
        "4. Open the raster file using Rasterio:\n",
        "   \n",
        "   ```python\n",
        "   with rasterio.open(url) as src:\n",
        "   ```\n",
        "\n",
        "5. Calculate the transform matrix for the output raster:\n",
        "    \n",
        "   ```python\n",
        "       dst_transform, width, height = calculate_default_transform(\n",
        "           src.crs,\n",
        "           dst_crs,\n",
        "           src.width,\n",
        "           src.height,\n",
        "           *src.bounds,\n",
        "       )\n",
        "   ```\n",
        "\n",
        "6.  Set properties for the output raster:\n",
        "\n",
        "   ```python\n",
        "       dst_kwargs = src.meta.copy()\n",
        "       dst_kwargs.update(\n",
        "           {\n",
        "               \"crs\": dst_crs,\n",
        "               \"transform\": dst_transform,\n",
        "               \"width\": width,\n",
        "               \"height\": height,\n",
        "               \"nodata\": 0,\n",
        "           }\n",
        "       )\n",
        "   ```\n",
        "\n",
        "7.  Create the output raster file and reproject each band:\n",
        "   \n",
        "   ```python\n",
        "       with rasterio.open(\"./temp/rgb_byte_local.tif\", \"w\", **dst_kwargs) as dst:\n",
        "           for i in range(1, src.count + 1):\n",
        "               reproject(\n",
        "                   source=rasterio.band(src, i),\n",
        "                   destination=rasterio.band(dst, i),\n",
        "                   src_transform=src.transform,\n",
        "                   src_crs=src.crs,\n",
        "                   dst_transform=dst_transform,\n",
        "                   dst_crs=dst_crs,\n",
        "                   resampling=Resampling.nearest,\n",
        "               )\n",
        "   ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.warp import reproject, Resampling, calculate_default_transform\n",
        "\n",
        "\n",
        "dst_crs = \"EPSG:3857\"  # web mercator(ie google maps)\n",
        "\n",
        "url = \"https://github.com/rasterio/rasterio/raw/main/tests/data/RGB.byte.tif\"\n",
        "\n",
        "with rasterio.open(url) as src:\n",
        "    src_transform = src.transform\n",
        "\n",
        "    # calculate the transform matrix for the output\n",
        "    dst_transform, width, height = calculate_default_transform(\n",
        "        src.crs,\n",
        "        dst_crs,\n",
        "        src.width,\n",
        "        src.height,\n",
        "        *src.bounds,  # unpacks outer boundaries (left, bottom, right, top)\n",
        "    )\n",
        "\n",
        "    # set properties for output\n",
        "    dst_kwargs = src.meta.copy()\n",
        "    dst_kwargs.update(\n",
        "        {\n",
        "            \"crs\": dst_crs,\n",
        "            \"transform\": dst_transform,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"nodata\": 0,  # replace 0 with np.nan\n",
        "        }\n",
        "    )\n",
        "\n",
        "    with rasterio.open(\"./temp/rgb_byte_local.tif\", \"w\", **dst_kwargs) as dst:\n",
        "        for i in range(1, src.count + 1):\n",
        "            reproject(\n",
        "                source=rasterio.band(src, i),\n",
        "                destination=rasterio.band(dst, i),\n",
        "                src_transform=src.transform,\n",
        "                src_crs=src.crs,\n",
        "                dst_transform=dst_transform,\n",
        "                dst_crs=dst_crs,\n",
        "                resampling=Resampling.nearest,\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting\n",
        "\n",
        "To manipulate and visualize rasters, we can use two packages: `rasterio` and `geowombat`. The latter is a collection of utilities commonly used in GIS.\n",
        "\n",
        "### geowowmbat\n",
        "\n",
        "GeoWombat provides utilities to process geospatial and time series of raster data at scale. Easily process Landsat, Sentinel, Planetscope or RGB data and others.\n",
        "\n",
        "Consult the [API reference](https://geowombat.readthedocs.io/en/latest/api.html)\n",
        "\n",
        "\n",
        "#### Plot Example Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "# Import GeoWombat\n",
        "import geowombat as gw\n",
        "\n",
        "# import plotting\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as pe\n",
        "\n",
        "from geowombat.data import l8_224077_20200518_B2 \n",
        "\n",
        "fig, ax = plt.subplots(dpi=200)\n",
        "\n",
        "with gw.open(l8_224077_20200518_B2,\n",
        "                band_names=['blue']) as src:\n",
        "    src.where(src != 0).sel(band='blue').plot.imshow(robust=True, ax=ax)\n",
        "plt.tight_layout(pad=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot a local File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!wget -cq --show-progress \"https://github.com/alexanderquispe/Diplomado_PUCP/raw/lecture14/Lecture_14/temp/LC08_20200518_webMC.tif\"\n",
        "\n",
        "fix, ax = plt.subplots(dpi=200)\n",
        "\n",
        "with gw.open(\n",
        "    \"/content/LC08_20200518_webMC.tif\",\n",
        "\n",
        ") as src:\n",
        "    src.where(src !=0).plot.imshow(robust=True, ax=ax)\n",
        "plt.tight_layout(pad=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rasterio\n",
        "\n",
        "You can use the base library, and there's also a wealth of documentation and help available on forums.\n",
        "\n",
        "More in [rasterio documentation](https://rasterio.readthedocs.io/en/stable/topics/reading.html)\n",
        "\n",
        "#### Plot Everything\n",
        "\n",
        "<!-- https://pygis.io/docs/f_rs_plot.html -->\n",
        "\n",
        "Original Projection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from rasterio.plot import show\n",
        "\n",
        "src = rasterio.open(url)\n",
        "show(src.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot single Band\n",
        "\n",
        "Reproject "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from rasterio.plot import show\n",
        "\n",
        "src = rasterio.open(\"./temp/rgb_byte_local.tif\")\n",
        "show(src.read(2), transform=src.transform, cmap=\"viridis\")  # 2 : Second band"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Editing Rasters and Remotely Sensed Data\n",
        "\n",
        "<!-- configuration for docker: https://pygis.io/docs/b_conda_started.html#the-easy-way -->\n",
        "\n",
        "<!-- https://pygis.io/docs/f_rs_edit.html#editing-rasters-and-remotely-sensed-data -->\n",
        "\n",
        "In this part of the content, we'll utilize `geowombat` as it provides simplicity in handling raster data.\n",
        "\n",
        "\n",
        "### Delete NAS\n",
        "\n",
        "The code to filter the raster is `src.where(src != 0)`, which can be used as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geowombat as gw\n",
        "from geowombat.data import l8_224078_20200518\n",
        "\n",
        "# Zeros are replaced with nans\n",
        "with gw.open(l8_224078_20200518) as src:\n",
        "    data = src.where(src != 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting  `no data` values\n",
        "\n",
        "To define missing data values, we can use the configuration manager within the open method parameter like with `gw.open(raster, nodata=0)`. To use it, we'll define `0` as `NA`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geowombat as gw\n",
        "from geowombat.data import l8_224078_20200518\n",
        "\n",
        "# Zeros are replaced with nans\n",
        "with gw.open(l8_224078_20200518, nodata=0) as src:\n",
        "    print('gw.open: ',src.attrs['nodatavals'])\n",
        "    #  replace 0 with nan\n",
        "    src=src.gw.mask_nodata() \n",
        "\n",
        "\n",
        "# Zeros are replaced with nans\n",
        "with gw.config.update(nodata=0):\n",
        "  with gw.open(l8_224078_20200518) as src:\n",
        "    print('gw.config',src.attrs['nodatavals'])\n",
        "    #  replace 0 with nan\n",
        "    src=src.gw.mask_nodata() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mutate values\n",
        "\n",
        "Similar to pandas, we can use the `.replace({old_value: new_value})` method to replace values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geowombat as gw\n",
        "from geowombat.data import l8_224078_20200518\n",
        "\n",
        "# Replace 1 with 10\n",
        "with gw.open(l8_224078_20200518) as src:\n",
        "    data = src.gw.replace({1: 10})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "o update values, we can use `{algorithm}(src)`  within the `with` statement identifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geowombat as gw\n",
        "from geowombat.data import l8_224078_20200518\n",
        "\n",
        "# Replace 1 with 10\n",
        "with gw.open(l8_224078_20200518) as src:\n",
        "    data = src * 0.001 + 80\n",
        "    print(data[0].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ## Remote Sensing Coordinate Reference Systems -->\n",
        "\n",
        "<!-- https://pygis.io/docs/f_rs_crs.html#remote-sensing-coordinate-reference-systems -->\n",
        "\n",
        "## Handle Multiple Remotely Sensed Images\n",
        "\n",
        "Combining raster files can sometimes be tedious, but with the utilities of `geowombat`, this can be relatively easy, considering that both rasters must match in the `crs`.\n",
        "\n",
        "### Raster Union\n",
        "\n",
        "To join 2 rasters or images, we can use the `mosaic=True` parameter within the `with` statement, and to overlap and merge them, we can use `overlap: string['mean', 'min', 'max']='mean'`. For clarity, we'll use `band_names=['blue']`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import GeoWombat\n",
        "import geowombat as gw\n",
        "\n",
        "# import plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as pe\n",
        "\n",
        "# load data\n",
        "from geowombat.data import l8_224077_20200518_B2, l8_224078_20200518_B2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's plot the rasters we have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, dpi=200)\n",
        "\n",
        "with gw.open(\n",
        "     [l8_224077_20200518_B2],\n",
        "     band_names=['blue'],\n",
        "\n",
        " ) as src:\n",
        "     src.where(src != 0).sel(band='blue').gw.imshow(robust=True, ax=ax1)\n",
        " \n",
        "plt.tight_layout(pad=1)\n",
        "\n",
        "with gw.open(\n",
        "     [l8_224078_20200518_B2],\n",
        "     band_names=['blue'],\n",
        "\n",
        " ) as src:\n",
        "     src.where(src != 0).sel(band='blue').gw.imshow(robust=True, ax=ax2)\n",
        " \n",
        "plt.tight_layout(pad=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the rasters have missing data. To join the 2 rasters, we can use the following syntax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fix, ax = plt.subplots(dpi=100)\n",
        "with gw.open(\n",
        "    [l8_224077_20200518_B2, l8_224078_20200518_B2],  # images, o raster path\n",
        "    band_names=[\"blue\"],  # band\n",
        "    mosaic=True,\n",
        "    bounds_by=\"union\",\n",
        ") as src:\n",
        "    (\n",
        "        src.where(src != 0)# dropna\n",
        "        .sel(band=\"blue\")  \n",
        "        .gw.imshow(robust=True, ax=ax)  # band = blue\n",
        "    )\n",
        "\n",
        "plt.tight_layout(pad=1)  # show img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Raster Intersection\n",
        "<!-- https://pygis.io/docs/f_rs_extraction.html#raster-data-extraction -->\n",
        "\n",
        "From the previous exercise, we see that we complete the 2 rasters with `bounds_by='union'` to obtain the raster that both share. We can use the parameter `bounds_by='intersection'` while keeping all other parameters the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fix, ax = plt.subplots(dpi=100)\n",
        "with gw.open(\n",
        "    [l8_224077_20200518_B2, l8_224078_20200518_B2],  # images, o raster path\n",
        "    band_names=[\"blue\"],  # band\n",
        "    mosaic=True,\n",
        "    bounds_by=\"intersection\",\n",
        ") as src:\n",
        "    (\n",
        "        src.where(src != 0)  # dropna\n",
        "        .sel(band=\"blue\")\n",
        "        .gw.imshow(robust=True, ax=ax)  # band = blue\n",
        "    )\n",
        "\n",
        "plt.tight_layout(pad=1)  # show img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raster Data Extraction\n",
        "\n",
        "\n",
        "Raster data is often of little use unless we can extract and summarize the data. For instance, extracting raster to points by coordinates allows us to pass data to machine learning models for land cover classification or cloud masking.\n",
        "\n",
        "In order to extract data, `geowombat` requires two elements: the raster containing the information and the coordinates from which data is to be obtained. It's crucial that both the raster and the coordinates share the same coordinate reference system (CRS), or else they need to be reprojected to align.\n",
        "\n",
        "To illustrate this, let's use some sample coordinates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import data and packages\n",
        "import geowombat as gw\n",
        "from geowombat.data import l8_224078_20200518\n",
        "\n",
        "# Coordinates in latitude/longitude\n",
        "lat, lon = -25.50142964, -54.39756038\n",
        "\n",
        "with gw.open(l8_224078_20200518) as src:\n",
        "    # Transform the coordinates to map units\n",
        "    x, y = gw.lonlat_to_xy(lon, lat, src)\n",
        "    # Transform the map coordinates to data indices\n",
        "    j, i = gw.coords_to_indices(x, y, src)\n",
        "    data = src[:, i, j].data.compute()\n",
        "\n",
        "print(data.flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a more generalized approach, we can use `gw.extract(point_coordinates)`, which returns a GeoPandas DataFrame. In the following example, `l8_224078_20200518_points` is a `'.gpkg'` file containing point coordinates, and df is a `GeoDataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geowombat as gw\n",
        "from geowombat.data import l8_224078_20200518, l8_224078_20200518_points\n",
        "\n",
        "with gw.open(l8_224078_20200518) as src:\n",
        "    df = src.gw.extract(l8_224078_20200518_points)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case we have a `GeoDataFrame` rather than a '.gpkg' file, we can use the following syntax. This involves having the dataframe (points in this case), transforming the CRS to match both the raster and the GeoDataFrame, and then extracting the values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geowombat as gw\n",
        "from geowombat.data import l8_224078_20200518, l8_224078_20200518_points\n",
        "import geopandas as gpd\n",
        "\n",
        "point_df = gpd.read_file(l8_224078_20200518_points) #geoDataFrame\n",
        "print(point_df.crs)\n",
        "\n",
        "# Transform the CRS to WGS84 lat/lon\n",
        "point_df = point_df.to_crs('epsg:4326')\n",
        "print(point_df.crs)\n",
        "\n",
        "# Extract data\n",
        "with gw.open(l8_224078_20200518) as src:\n",
        "    df = src.gw.extract(point_df)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have a dataset with coordinates that are not points but rather a shapefile, we can use the same previous syntax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from geowombat.data import l8_224078_20200518, l8_224078_20200518_polygons\n",
        "\n",
        "# with gw.config.update(sensor='bgr'):\n",
        "with gw.open(l8_224078_20200518) as src:\n",
        "    df = src.gw.extract(l8_224078_20200518_polygons)\n",
        "\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous plot might not be clear, so let's filter the data to focus on a specific shapefile, where each pixel is represented by a value in the GeoDataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.query(\"name == 'water'\").plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spatial Prediction using ML in Python\n",
        "\n",
        "<!-- https://pygis.io/docs/f_rs_ml_predict.html#spatial-prediction-using-ml-in-python -->\n",
        "\n",
        "\n",
        "\n",
        "### Land Use Classification using Geowombat & Sklearn\n",
        "\n",
        "Creating land cover classification is a common task for remotely sensed data. In this tutorial, we'll demonstrate how to train a machine learning (ML) model using raster data. These methods heavily rely on the `sklearn_xarray` package. For understanding the pipeline commands, please refer to their documentation and [examples](https://scikit-learn.org/stable/auto_examples/index.html#pipelines-and-composite-estimators).\n",
        "\n",
        "#### Supervised Classification in Python\n",
        "\n",
        "In the following example, we'll utilize Landsat data and some training data to train a supervised `sklearn` model. The initial step involves having land classifications for a set of points or polygons. For instance, we have three polygons with the classes ['water', 'crop', 'tree', 'developed']. The first step is to use `LabelEncoder` to convert these classes into integer-based categories, stored in a new column called 'lc'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geowombat as gw\n",
        "from geowombat.data import l8_224078_20200518, l8_224078_20200518_polygons\n",
        "from geowombat.ml import fit, predict, fit_predict\n",
        "import geopandas as gpd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# The labels are string names, so here we convert them to integers\n",
        "labels = gpd.read_file(l8_224078_20200518_polygons)\n",
        "labels[\"lc\"] = le.fit(labels.name).transform(labels.name)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After converting the labels, we'll generate our `sklearn` pipeline, which includes normalization, principal component analysis (PCA) for dimensionality reduction, and a Gaussian Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use a data pipeline\n",
        "pl = Pipeline([('scaler', StandardScaler()),\n",
        "               ('pca', PCA()),\n",
        "               ('clf', GaussianNB())])\n",
        "\n",
        "fig, ax = plt.subplots(dpi=200, figsize=(5, 5))\n",
        "\n",
        "# Fit the classifier\n",
        "with gw.config.update(ref_res=150):\n",
        "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
        "        X, Xy, clf = fit(src, pl, labels, col=\"lc\")\n",
        "        y = predict(src, X, clf)\n",
        "        y.plot(robust=True, ax=ax)\n",
        "plt.tight_layout(pad=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, to fit and predict the model in one step, we use `fit_predict`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from geowombat.ml import fit_predict\n",
        "fig, ax = plt.subplots(dpi=200, figsize=(5, 5))\n",
        "\n",
        "with gw.config.update(ref_res=150):\n",
        "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
        "        y = fit_predict(src, pl, labels, col='lc')\n",
        "        y.plot(robust=True, ax=ax)\n",
        "plt.tight_layout(pad=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Unsupervised Classification in Python\n",
        "\n",
        "Unsupervised classification takes a different approach. Here, we don’t provide examples of different land cover types. Instead, we rely on the algorithm to identify distinct clusters of similar data and apply a unique label to each cluster. For example, water and trees will look very different in terms of their reflectance properties. Water reflects more blue and absorbs all the near-infrared, while trees reflect little blue and a lot of near-infrared. \n",
        "\n",
        "Therefore, water and trees should cluster together when plotted based on their different reflectances. These clusters will be assigned a unique value to each pixel (e.g., water = 1 and trees = 2), and later, the end user will need to assign the label to each numbered cluster (e.g., water = 1, trees = 2).\n",
        "\n",
        "\n",
        "In this example, we'll use KMeans to perform clustering. We need to decide beforehand how many clusters we want to identify. Typically, it's recommended to roughly double the number of expected classes and then recombine them later into the desired labels. This helps in better understanding and categorizing the variation in your image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "cl = Pipeline([('clf', KMeans(n_clusters=6, random_state=0))])\n",
        "\n",
        "fig, ax = plt.subplots(dpi=200, figsize=(5, 5))\n",
        "\n",
        "# Fit_predict unsupervised classifier\n",
        "with gw.config.update(ref_res=150):\n",
        "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
        "        y = fit_predict(src, cl)\n",
        "        y.plot(robust=True, ax=ax)\n",
        "plt.tight_layout(pad=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, we can observe that it effectively labels different clusters of data, and now it's up to us to determine which clusters should be categorized as water, trees, fields, etc.\n",
        "\n",
        "### Spatial Prediction with Time Series Stack\n",
        "\n",
        "When working with a stack of time series data, applying the same method as described previously is straightforward. However, we need to open multiple images, set `stack_dim` to 'time', and specify the `time_names`. Below, we're pretending to have two dates of Landsat imagery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(dpi=200, figsize=(5, 5))\n",
        "\n",
        "with gw.config.update(ref_res=150):\n",
        "    with gw.open(\n",
        "        [l8_224078_20200518, l8_224078_20200518],\n",
        "        time_names=[\"t1\", \"t2\"],\n",
        "        stack_dim=\"time\",\n",
        "        nodata=0,\n",
        "    ) as src:\n",
        "        y = fit_predict(src, pl, labels, col=\"lc\")\n",
        "        print(y)\n",
        "        # plot one time period prediction\n",
        "        y.sel(time=\"t1\").plot(robust=True, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above code performs spatial prediction with a time series stack. It opens multiple images, sets the stack dimension to 'time', and specifies the time names. After fitting and predicting the model, it plots the prediction for one time period.\n",
        "\n",
        "If you want to perform more sophisticated model tuning using sklearn, you can break up your fit and predict steps as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(dpi=200, figsize=(5, 5))\n",
        "\n",
        "with gw.config.update(ref_res=150):\n",
        "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
        "        X, Xy, clf = fit(src, pl, labels, col=\"lc\")\n",
        "        y = predict(src, X, clf)\n",
        "        y.plot(robust=True, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code demonstrates how to perform model tuning using sklearn. It fits the model, makes predictions, and plots the results.\n",
        "\n",
        "### Evaluating Model Performance and Hyperparameter Tuning\n",
        "\n",
        "Assessing model performance is crucial for successfully building a model. Sklearn provides various built-in tools for this purpose. One common method is k-fold cross-validation, where the data is divided into independent sets of training and testing data multiple times. This allows us to assess the model's ability to predict outcomes on unseen data.\n",
        "\n",
        "In this scenario, we'll use the supervised classification pipeline `pl` from earlier and perform cross-validation using k-fold. To use k-fold with Geowombat, we need to employ `CrossValidatorWrapper`, as shown in the example below, to work with xarray objects.\n",
        "\n",
        "Hyperparameter tuning is also essential. For example, we may need to determine the number of PCA components or experiment with scaling the data range using `StandardScaler`. To perform hyperparameter tuning with `GridSearchCV` in a pipeline, we need to set up the parameter grid. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pl = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"clf\", GaussianNB())])\n",
        "\n",
        "param_grid = {\"scaler__with_std\": [True, False], \"pca__n_components\": [1, 2, 3]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the `param_grid` dictionary, each step in the pipeline is labeled (e.g., 'scaler', 'pca', 'clf'). We reference these steps by name to try out different parameters for each step. For instance, `\"pca__n_components\": [1, 2, 3]` indicates that we'll experiment with three different values for the `n_components` parameter in the PCA step to choose the one that best predicts our testing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn_xarray.model_selection import CrossValidatorWrapper\n",
        "\n",
        "pl = Pipeline([('scaler', StandardScaler()),\n",
        "               ('pca', PCA()),\n",
        "               ('clf', GaussianNB())])\n",
        "\n",
        "cv = CrossValidatorWrapper(KFold())\n",
        "gridsearch = GridSearchCV(pl, cv=cv, scoring='balanced_accuracy',\n",
        "                    param_grid={\n",
        "                      \"scaler__with_std\":[True,False],\n",
        "                      \"pca__n_components\": [1, 2, 3]\n",
        "                      })\n",
        "\n",
        "fig, ax = plt.subplots(dpi=200,figsize=(5,5))\n",
        "\n",
        "with gw.config.update(ref_res=150):\n",
        "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
        "        # fit a model to get Xy used to train model\n",
        "        X, Xy, pipe = fit(src, pl, labels, col=\"lc\")\n",
        "\n",
        "        # fit cross valiation and parameter tuning\n",
        "        # NOTE: must unpack * object Xy\n",
        "        gridsearch.fit(*Xy)\n",
        "        print(gridsearch.cv_results_)\n",
        "        print(gridsearch.best_score_)\n",
        "        print(gridsearch.best_params_)        \n",
        "\n",
        "        # get set tuned parameters and make the prediction\n",
        "        # Note: predict(gridsearch.best_model_) not currently supported\n",
        "        pipe.set_params(**gridsearch.best_params_)\n",
        "        y = predict(src, X, pipe)\n",
        "        y.plot(robust=True, ax=ax)\n",
        "plt.tight_layout(pad=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to create a model with the optimal parameters we need to use `gridsearch.best_params_`, which holds a dictionary of each parameter and its optimal value. To `use` these values we need to update the parameters held in our returned pipeline, pipe, by using the `.set_params` method."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "py3_github_update",
      "language": "python",
      "display_name": "py3_github_update"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}